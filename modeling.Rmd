---
title: "170B"
author: "Jie Ma"
date: "5/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#if (!require(devtools)) install.packages("devtools")
```

```{r}
devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
```


```{r pressure, echo=FALSE}
#install.packages("furrr")
#install.packages("purrr")
#install.packages("ggthemes")
#install.packages("broom")
#install.packages("wordcloud")
#install.packages("tidytext")
#install.packages("tidystm")
library(stm)
library(furrr)
library(purrr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggthemes)
library(broom)
library(wordcloud)
library(tidytext)
library(tidystm)
#plan(multiprocess)
```

```{r pressure, echo=FALSE}
data <- read.csv("tableTS_1.csv")
processed <- textProcessor(data$Transcripts, metadata = data)
```
```{r pressure, echo=FALSE}
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
```
```{r pressure, echo=FALSE}
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```

```{r pressure, echo=FALSE}
poliblogPrevFit <- stm(documents = out$documents, vocab = out$vocab,
                        K = 20,
                        max.em.its = 75, data = out$meta,
                        init.type = "Spectral")

```


```{r pressure, echo=FALSE}
many_models <- data.frame(K = c(10, 20, 30, 40, 50, 60, 70, 80, 100)) %>% 
  mutate(topic_model = future_map(K, ~stm(documents = out$documents, vocab = out$vocab, K = .,
                                          verbose = FALSE)))

```

```{r pressure, echo=FALSE}
heldout <- make.heldout(documents = out$documents, vocab = out$vocab)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         #semantic_coherence = map(topic_model, semanticCoherence, documents = out$documents, vocab = out$vocab),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         #residual = map(topic_model, checkResiduals, documents = out$documents, vocab = out$vocab),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result

```

```{r pressure, echo=FALSE}
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            #Residuals = map_dbl(residual, "dispersion"),
            #`Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 20")

```

```{r pressure, echo=FALSE}
labelTopics(poliblogPrevFit, c(7))
```
```{r pressure, echo=FALSE}
plot(poliblogPrevFit, type = "summary", xlim = c(0, .3))
```
```{r pressure, echo=FALSE}
data2 <- read.csv("full_405.csv")
```

```{r pressure, echo=FALSE}
processed2 <- textProcessor(data2$content, metadata = data2)
```

```{r pressure, echo=FALSE}
out2 <- prepDocuments(processed2$documents, processed2$vocab, processed2$meta)
```

```{r pressure, echo=FALSE}
docs2 <- out2$documents
vocab2 <- out2$vocab
meta2 <-out2$meta
```


```{r pressure, echo=FALSE}
poliblogPrevFit2 <- stm(documents = out2$documents, vocab = out2$vocab,
                        K = 20,
                        max.em.its = 75, data = out2$meta,
                        init.type = "Spectral")
```
```{r pressure, echo=FALSE}
labelTopics(poliblogPrevFit2)
```

```{r pressure, echo=FALSE}
plot(poliblogPrevFit2, type = "summary", xlim = c(0, .3))
```

```{r pressure, echo=FALSE}
data3 <- read.csv("portion_405.csv")
```

```{r pressure, echo=FALSE}
processed3 <- textProcessor(data3$content, metadata = data3)
```


```{r pressure, echo=FALSE}
out3 <- prepDocuments(processed3$documents, processed3$vocab,
  processed3$meta, lower.thresh = 30)
```

```{r pressure, echo=FALSE}
poliblogPrevFit3 <- stm(documents = out3$documents, vocab = out3$vocab,
                        K = 20,
                        max.em.its = 75, data = out3$meta,
                        init.type = "Spectral")
```

```{r pressure, echo=FALSE}
labelTopics(poliblogPrevFit3)
```

```{r pressure, echo=FALSE}
plot(poliblogPrevFit3, type = "summary", xlim = c(0, .3))
```


```{r pressure, echo=FALSE}
cloud(poliblogPrevFit3, topic=5)
```



```{r pressure, echo=FALSE}
td_beta <- tidy(poliblogPrevFit3)
td_beta
```

```{r pressure, echo=FALSE}
td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with different topics")
```


```{r pressure, echo=FALSE}
data4 <- read.csv("only_state.csv")
```


```{r pressure, echo=FALSE}
processed4 <- textProcessor(data4$transcript, metadata = data4)
```

```{r pressure, echo=FALSE}
out4 <- prepDocuments(processed4$documents, processed4$vocab,
  processed4$meta, lower.thresh = 30)
```


```{r pressure, echo=FALSE}
head(out4)
```

```{r pressure, echo=FALSE}
poliblogPrevFit4 <- stm(documents = out4$documents, vocab = out4$vocab,
                        K = 20, prevalence =~ state,
                        max.em.its = 75, data = out4$meta,
                        init.type = "Spectral")
```

```{r pressure, echo=FALSE}
summary(poliblogPrevFit4)
```



```{r pressure, echo=FALSE}
out4$meta$state <- as.factor(out4$meta$state)
prep <- estimateEffect(1:20 ~ state, poliblogPrevFit4, meta=out4$meta, 
                       uncertainty="Global")
```

```{r pressure, echo=FALSE}
out4$meta$state <- as.factor(out4$meta$state)
prep <- estimateEffect(1:20 ~ state, poliblogPrevFit4, meta=out4$meta, 
                       uncertainty="Global")
effect <- extract.estimateEffect(prep, "state", model = poliblogPrevFit4, method = "pointestimate")
```

```{r}
p1 <- ggplot(effect, aes(x = estimate, y = covariate.value, fill=topic)) + 
  facet_wrap(~topic)
p1
```



```{r}
plot(prep, "state", method = "pointestimate", topics = 19, model = poliblogPrevFit4, printlegend = FALSE, xaxt = "n", xlab = "Expected porportion", ylab = "States")
```


```{r}
effectT19 <- filter(effect, topic == 19)

ggplot(effectT19, aes(x=estimate, y=covariate.value)) + 
    geom_point() + 
    geom_errorbar(aes(xmin = ci.lower, xmax = ci.upper)) +
    xlab("Expected Porportion")

```

```{r}
plot(poliblogPrevFit4,type="hist")
```
```{r pressure, echo=FALSE}
data5 <- read.csv("portion_state_date.csv")
```


```{r pressure, echo=FALSE}
processed5 <- textProcessor(data5$transcript_portion, metadata = data5)
```

```{r pressure, echo=FALSE}
out5 <- prepDocuments(processed5$documents, processed5$vocab,
  processed5$meta, lower.thresh = 30)
```

```{r pressure, echo=FALSE}
many_models2 <- data.frame(K = c(10, 20, 30, 40, 50, 60, 70, 80,90, 100)) %>% 
  mutate(topic_model = future_map(K, ~stm(documents = out5$documents, vocab = out5$vocab, K = .,
                                          verbose = FALSE)))

```

```{r pressure, echo=FALSE}
heldout2 <- make.heldout(documents = out5$documents, vocab = out5$vocab)

k_result2 <- many_models2 %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         #semantic_coherence = map(topic_model, semanticCoherence, documents = out$documents, vocab = out$vocab),
         eval_heldout = map(topic_model, eval.heldout, heldout2$missing),
         #residual = map(topic_model, checkResiduals, documents = out$documents, vocab = out$vocab),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result2

```

```{r pressure, echo=FALSE}
k_result2 %>%
  transmute(K,
            `Lower bound` = lbound,
            #Residuals = map_dbl(residual, "dispersion"),
            #`Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 20")

```

```{r}
findingk <- searchK(out5$documents, out5$vocab, K = c(10:30), prevalence =~ state, data = out5$meta, verbose=FALSE)
```

```{r}
plot(findingk)
```
Within certain boundaries, it seems that the choice of model is a matter of trade-offs. In our case, the best results seem to be in the range 10-20. It can be helpful to compare then semantic coherence to exclusivity, as models with fewer topics have higher semantic coherence for more topics, but lower exclusivity. To check for it however we have to fit the models first, which is what we do next. We will set the initiatlization method to the default “Spectral”, as advised by the author of the package, although alternatives are available (the vignette offers further information about the different methods of initialization). Also in this case the post of Julia Silge mentioned above presents an alternative procedure.

```{r pressure, echo=FALSE}
poliblogPrevFit5 <- stm(documents = out5$documents, vocab = out5$vocab,
                        K = 15, prevalence =~ state,
                        max.em.its = 75, data = out5$meta,
                        init.type = "Spectral")
```

```{r pressure, echo=FALSE}
labelTopics(poliblogPrevFit5)
```

```{r pressure, echo=FALSE}
plot(poliblogPrevFit5, type = "summary", xlim = c(0, .3))
```

1,amend, bill, senate
2, health
4, age, young, health, new jersey
5, cigarette tax
6, marijuana?
8, substitue change
9, label
10, smoke, effect, nicotin
14, school, youth


```{r pressure, echo=FALSE}
cloud(poliblogPrevFit5, topic=4)
```

```{r pressure, echo=FALSE}
out5$meta$state <- as.factor(out5$meta$state)
prep <- estimateEffect(1:15 ~ state, poliblogPrevFit5, meta=out5$meta)
effect <- extract.estimateEffect(prep, "state", model = poliblogPrevFit5, method = "pointestimate")
```

32 states in total
topic 4 new jersey
topic 5 tax, discussed in PA, NC
topic 7 e-cig, most of the states
topic 9 product, manufactor, label regulation most of the states
topic 10 all states
topic 14 youth all states
topic 15 HI
```{r}
effectT10 <- filter(effect, topic == 10)

ggplot(effectT10, aes(x=estimate, y=covariate.value)) + 
    geom_point() + 
    geom_errorbar(aes(xmin = ci.lower, xmax = ci.upper)) +
    xlab("Expected Proportion") +
    ylab("state") + 
    ggtitle("Expected Proportion of Topic 10 Estimated by Metadata State")
```
